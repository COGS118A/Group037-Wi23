{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COGS 118A- Project Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Names\n",
    "\n",
    "- Kalam Park\n",
    "- Kira Fleischer \n",
    "- Cray Minor "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract \n",
    "\n",
    "The goal of the reserach is to take advatnage of Lichess's online recorded matches to determine the factors that most strongly influences the outcome of the match and to build a model that is able to predict the winner based on the factors found. The data used in this project represents various quantitative and qualitative measures of the competitors (~20,000 matches), in which we will use Logistic Regression to filter out the significant factors and create a filtered dataset for the model to be trained upon. We currently plan to use the SVM and KNN algorithms to select the model whilst cross-validating the model using metrics such as precision, recall, F1 score, and accuracy. This research aims to contribute to the understanding of complex competition, as well as assist in developing game strategies that closely links to success."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "Chess is one of the world's most popular strategy games to date, and its ever-growing popularity has been heavily expedited due to the growing number of online platforms (Chess.com, Lichess) and users. With numerous events, streams, tutors, and creators online, chess can now be played and learned virtually anytime, anywhere. With the insurmountable amount of variations, openings, and data that comes with chess, the average player likely just scracthes the surface in understanding the deep complexity the game consists of. The emergence of AI and machine learning have offered a solution to this common dilemma, by providing a way to compute the mass amounts of available data and train the computer to do the thinking for them. That being said, AI and chess bots are amongst the most well known examples, popular for its near perfect chess performance, as well as a way of cheating.<a name=\"lorenz\"></a>[<sup>[1]</sup>](#lorenznote) There has also been a lot of research on chess analysis and playing, such as Stockfish project, an opensource chess analysis algorithm which can provide players with an after game review with one of the most powerful chessbots.<a name=\"lorenz\"></a>[<sup>[2]</sup>](#lorenznote) Evidently such knowledge can be highly beneficial in strategizing moves, openings, and sequences.\n",
    "\n",
    "Similarly, our proposed work aims to provide helpful information by analyzing 20,000 Lichess matches and highlighting the variables that most heavily influences a user to win the match. Upon gathering these factors, we plan to restructure and split the dataset in order to train a model into predicting a winner based on some user inputs. This in hopes will help users strategize between openings, first moves, and help determine if white pieces really have an inherent advantage in novice to professional level matches. <a name=\"lorenz\"></a>[<sup>[3]</sup>](#lorenznote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "Each side starting with 16 pieces, 6 different roles, and 64 squares there are nearly 10^120 different chess game variations, also known as Shannon's Number, which is more than there are atoms in the observable universe. <a name=\"lorenz\"></a>[<sup>[4]</sup>](#lorenznote) We will be looking at the different features contained within our data to see the highest correlated with a win for white(1) or a win for black(0). Logistic Regression will serve as our baseline binary classifier because of its simplicity and replicability. We will compare its performance to more complex algorithms such as Random Forests and Support Vector Machinees quantified using confusion matricies and a ROC curve. After evaluation we select the model that performs best on the test and validation sets and its ability to generalize, to provide a comprehensive analysis of these games and potential winning strategies. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "### Chess Game Dataset - Lichess (https://www.kaggle.com/datasets/datasnaek/chess)\n",
    "- A collection of ~20,000 recorded chess matches between two Lichess users \n",
    "- Contains 16 variables \n",
    "    - Game ID; \n",
    "    - Rated (T/F);\n",
    "    - Start Time;\n",
    "    - End Time;\n",
    "    - Number of Turns;\n",
    "    - Game Status;\n",
    "    - Winner;\n",
    "    - Time Increment;\n",
    "    - White Player ID;\n",
    "    - White Player Rating;\n",
    "    - Black Player ID;\n",
    "    - Black Player Rating;\n",
    "    - All Moves in Standard Chess Notation;\n",
    "    - Opening Eco (Standardised Code for any given opening, list here);\n",
    "    - Opening Name;\n",
    "    - Opening Ply (Number of moves in the opening phase)\n",
    "\n",
    "\n",
    "- Evidently some varibales will be more impactful than others, some critical ones are as listed:\n",
    "    - White/Black rating (ranking)\n",
    "    - Opening move \n",
    "    - Piece colors\n",
    "\n",
    "A lot of the categorzing is done for us since each opening move has a respective eco code. There are some varibales such as Game status and Winner that can be categorized numerically. We also plan to just completely remove some variables we believe should have no impact or influence on predicting who would win a game (ex: Game status, All Moves in Standard Chess Notation, Game ID, player IDs, etc.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposed Solution\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to predict the winner of the chess game, we will compare three different classification models and select the best one based on their respective performances, as measured by the evaluation metrics accuracy and f1 score. The three models we will use are:\n",
    "\n",
    "1. Logistic Regression\n",
    "\n",
    "Logistic regression is a method that aims to fit a linear decision boundary that separates two classes based on their input features. Since a white win is equivalent to a black loss, our two classes will only be in terms of white: white win is the positive class (1), and white loss is the negative class (0). In order to implement this model, we will split our dataset into training, validation, and test sets, where we will first train the model, evaluate it based on accuracy and f1 score, tune the hyperparameters using the validation set, and finally predict classifications (white win or white loss) using the test set. We will use gradient descent to find the best hyperparameters that minimize the loss function.\n",
    "\n",
    "2. Random Forests\n",
    "\n",
    "Random forests is a method that combines multiple decision trees that are each trained on a random subset of the training data and a random subset of the input features. We are using random forests because this method has many advantages over simple decision trees. For instance, random forests reduce overfitting by averaging the predictions of many decision trees. More importantly, however, random forests provide a measure of the importance of each feature by evaluating how much the performance of the model changes when different features are removed. This will help us determine which feature (player ratings, opening moves, piece color, etc.) are most important in predicting a winner.\n",
    "\n",
    "3. SVM\n",
    "\n",
    "SVM is a method that aims to find a hyperplane that separates two classes while creatin the largest margin between the two classes, which is determined by support vectors within both classes. This is achieved by minimizing a loss function that accounts for both misclassification errors (white predicted to win but white actually loses) and points that are within the margin surrounding the decision boundary hyperplane. We will most likely use linear SVM since this handles large datasets well, and we do not anticipate any curvature to or nonlinear patters in the data. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate our model performances using the metrics accuracy and f1 score. Accuracy will be calculated by dividing the the number of correct predictions (white predicted to win and white actually wins, and white predicted to lose and white actually loses) by the number of total predictions (the total training set size). This will give a pretty good estimate of which model is best at accurately predicting who will win since this is the most direct computation. Additionally, we will calculate the f1 score to compare our models. This is done by dividing the product of precision and recall by the sum of precision and recall. Precision is calculated by dividing the true positives (white predicted to win and white actually wins) by the number of predicted positives (white predicted to win and white actually wins + white predicted to win but black actually wins). Recall is calculated by dividing the true positives by the number of positives (white predicted to win and white actually wins + black predicted to win but white actually wins). We wanted to use the f1 score since the way that we designed the binary classes only relates to white, so we wanted to use a metric that combines both minimizing false positives and false negatives, since this accounts for black wins and white wins equally:\n",
    "\n",
    "false positive : white predicted win, black actually wins = black predicted loss, white actually loss\n",
    "\n",
    "false negative : white predicted loss, black actually loss = black predicted win, white actually win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethics & Privacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to address the ethical concern of privacy, we will delete the Game ID and player ID variables since these variables will not impact our prediction of who will win. This way, there will be no identifying information from either of the players, so they will remain anonymous and their privacy will be protected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team Expectations "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Meet weekly to discuss what we have each accomplished and what we still need to work on\n",
    "* Notify other group members of what we are currently working on and when we have completed it\n",
    "* Communicate through text and Discord\n",
    "* Respond to communications within 24 hours, not including weekends\n",
    "* Ask for help when needed and work together to help address any confusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Timeline Proposal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Meeting Date  | Meeting Time| Completed Before Meeting  | Discuss at Meeting |\n",
    "|---|---|---|---|\n",
    "| 2/20  |  1 PM |  Brainstorm topics/questions (all)  | Determine best form of communication; Discuss and decide on final project topic; find dataset | \n",
    "| 2/21  |  9 AM |  Do background research on topic (Cray); finalize dataset (Kalam)| Draft project proposal | \n",
    "| 2/22  | 5 PM  | Edit, finalize, and submit proposal  |  Assign group members to lead each specific part   |\n",
    "| 3/1  | 5 PM  | Import & Wrangle Data, do some EDA (Kalam) | Review/Edit wrangling/EDA; Discuss Analysis Plan   |\n",
    "| 3/8  | 5 PM  | Finalize wrangling/EDA; Begin programming for project (Kira) | Discuss/edit project code; Complete project |\n",
    "| 3/15  | 5 PM  | Complete analysis; Draft results/conclusion/discussion (Cray)| Discuss/edit full project |\n",
    "| 3/22  | Before 11:59 PM  | NA | Turn in Final Project  |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Footnotes\n",
    "<a name=\"lorenznote\"></a>1.[^](#lorenz): The Impact of Artificial Intelligence on the Chess World, Duca Iliescu (2020) https://games.jmir.org/2020/4/e24049/\n",
    "\n",
    "<a name=\"lorenznote\"></a>2.[^](#lorenz): AlphaZero, Zhang & Yu (2020) https://link.springer.com/chapter/10.1007/978-981-15-4095-0_15\n",
    "\n",
    "<a name=\"lorenznote\"></a>3.[^](#lorenz): First Move Advantage in Chess https://en.wikipedia.org/wiki/First-move_advantage_in_chess\n",
    "\n",
    "<a name=\"lorenznote\"></a>4.[^](#lorenz): Which is greater? The number of atoms in the universe or the number of chess moves?, Kiernan https://www.liverpoolmuseums.org."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
